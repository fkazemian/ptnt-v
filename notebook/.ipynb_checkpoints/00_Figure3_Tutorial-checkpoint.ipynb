{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "748213d4",
   "metadata": {},
   "source": [
    "# Figure 3 Tutorial · Self‑Consistent, Non‑Markovian Characterization (PTNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc6ddb",
   "metadata": {},
   "source": [
    "\n",
    "### How to run this notebook\n",
    "- Ensure you can import `ptnt`. From the repo root: `pip install -e .`\n",
    "- **CPU** is fine. For **GPU (JAX)**, verify `nvidia-smi`, then `pip install \"jax[cuda12]\"`.\n",
    "- First JAX call compiles with XLA — a short one‑time warm‑up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make a nearby PTNT checkout importable if not pip-installed.\n",
    "import os, sys, pathlib\n",
    "roots = [pathlib.Path.cwd(), *pathlib.Path.cwd().parents]\n",
    "for r in roots[:4]:\n",
    "    if (r / \"ptnt\").is_dir() and str(r) not in sys.path:\n",
    "        sys.path.insert(0, str(r))\n",
    "\n",
    "import importlib\n",
    "try:\n",
    "    import ptnt\n",
    "    from ptnt._version import __version__ as ptnt_version\n",
    "    print(\"[ptnt] import OK → version:\", ptnt_version)\n",
    "except Exception as e:\n",
    "    print(\"[ptnt] import failed:\", e)\n",
    "    raise\n",
    "\n",
    "# Optional: show JAX devices (CPU/GPU)\n",
    "try:\n",
    "    import jax\n",
    "    print(\"[ptnt] JAX devices:\", jax.devices())\n",
    "except Exception as e:\n",
    "    print(\"[ptnt] JAX not available:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2cccd",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Configuration for Figure 3 (YAML)\n",
    "We’ll load the built‑in **Figure 3** config via `default_config_for_experiment(\"figure3\")` and **explain the key knobs**.\n",
    "If you ship a curated `configs/figure3.yaml` in your repo, it will match those defaults; users can alter the\n",
    "**data sizes** and **training** settings here or in a copy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ptnt.io.run import default_config_for_experiment\n",
    "import copy, json\n",
    "cfg_full = default_config_for_experiment(\"figure3\")\n",
    "\n",
    "# Show a trimmed view for readability\n",
    "def view(cfg):\n",
    "    keep = [\"pt\", \"device\", \"data\", \"training\", \"output\"]\n",
    "    compact = {k: cfg.get(k, {}) for k in keep}\n",
    "    print(json.dumps(compact, indent=2))\n",
    "view(cfg_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79322a6a",
   "metadata": {},
   "source": [
    "### Your repository’s `configs/figure3.yaml`\n",
    "```yaml\n",
    "# Reproduces \"Figure 3\" benchmark: NM-GST (X-decomposition) vs Standard ptnt\n",
    "seed: 123\n",
    "device:\n",
    "  backend: \"aer_simulator_density_matrix\"\n",
    "  basis_gates: [\"cx\", \"rz\", \"sx\", \"x\"]\n",
    "  noise:\n",
    "    depolarizing_p1q: 0.002\n",
    "    coherent_rx_angle: 0.09817477042468103    # pi/32\n",
    "pt:\n",
    "  n_qubits: 1\n",
    "  n_steps: 9\n",
    "  template: \"dd_clifford\"  # see ptnt.circuits.templates\n",
    "  env_IA: {rxx: 0.20943951, ryy: 0.20943951, rzz: 0.31415927}  # pi/15, pi/15, pi/10\n",
    "  crosstalk_IA: {rxx: 0.0, ryy: 0.0, rzz: 0.39269908}          # pi/8\n",
    "data:\n",
    "  n_jobs: 20\n",
    "  shadows_per_job: 300\n",
    "  shots_per_char: 1024\n",
    "  val_shadows: 300\n",
    "  shots_per_val: 16384\n",
    "training:\n",
    "  mode: \"X_decomp\"   # [\"X_decomp\", \"normal\"]\n",
    "  epochs: 600\n",
    "  batch_size: 1000\n",
    "  kappa: 1.0\n",
    "  optimizer: \"adam\"\n",
    "  autodiff: \"jax\"\n",
    "  causality_key_size: 200\n",
    "  horizontal_bonds: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "  vertical_bonds:\n",
    "    - [1]\n",
    "    - [1]\n",
    "    - [1]\n",
    "    - [1]\n",
    "    - [1]\n",
    "    - [1]\n",
    "    - [1]\n",
    "    - [1]\n",
    "    - [1]\n",
    "  K_lists: [[1,1,1,1,1,1,1,1,1,1]]\n",
    "output:\n",
    "  dir: \"runs/figure3\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6045d7e",
   "metadata": {},
   "source": [
    "\n",
    "**Key sections**\n",
    "- `pt:`\n",
    "  - `n_qubits` (Q): number of *system* qubits (env ancilla is added automatically).\n",
    "  - `n_steps` (T): number of time steps (local layer + env coupling per step, plus a final local layer).\n",
    "  - `template`: here, `\"dd_clifford\"` (baseline shell).\n",
    "  - `env_IA`: XYZ coupling strengths for the **environment–system** unitary each step.\n",
    "- `device:`\n",
    "  - `backend`: e.g., `\"aer_simulator\"` for Qiskit Aer.\n",
    "  - `basis_gates`: can be `null` to let backend decide (recommended for backend‑aware transpilation).\n",
    "  - Optional `noise` block to compose **depolarizing + coherent** errors on `sx`.\n",
    "- `data:`\n",
    "  - `jobs`, `shadows_per_job`, `shots_per_char`: characterization (training) workload.\n",
    "  - `val_shadows`, `shots_per_val`: validation workload (more shots → tighter entropy floor).\n",
    "- `training:`\n",
    "  - `epochs`, `batch_size`: stochastic MLE settings.\n",
    "  - `kappa`: causality regularization weight (start small; tune).\n",
    "  - `mode`: `\"normal\"` (Full‑U view) or `\"X_decomp\"` (RZ view with fixed X‑decomp factor).\n",
    "  - `opt`: contraction optimizer, e.g., `\"greedy\"`, `\"auto-hq\"`, or `\"hyper-kahypar\"` if installed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0a3d5",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Small “dry‑run” (fast) vs. Full run (heavier)\n",
    "We start with a **small demo** so users can complete a full loop quickly. Then we provide a cell\n",
    "that flips into your **full** Figure‑3 settings for higher fidelity reproduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e9ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ptnt.io.run import run_from_config, default_config_for_experiment\n",
    "from pathlib import Path\n",
    "\n",
    "# Load defaults and then downsize for a fast pass\n",
    "cfg = default_config_for_experiment(\"figure3\")\n",
    "\n",
    "# --- FAST SETTINGS (edit or comment to scale up) ---\n",
    "cfg[\"training\"][\"epochs\"] = cfg[\"training\"].get(\"epochs\", 3) // 3 or 1\n",
    "cfg[\"training\"][\"batch_size\"] = max(64, cfg[\"training\"].get(\"batch_size\", 256)//2)\n",
    "cfg[\"data\"][\"jobs\"] = max(2, cfg[\"data\"].get(\"jobs\", 20)//10)\n",
    "cfg[\"data\"][\"shadows_per_job\"] = max(60, cfg[\"data\"].get(\"shadows_per_job\", 300)//5)\n",
    "cfg[\"data\"][\"shots_per_char\"] = max(256, cfg[\"data\"].get(\"shots_per_char\", 1024)//4)\n",
    "cfg[\"data\"][\"val_shadows\"] = max(20, cfg[\"data\"].get(\"val_shadows\", 300)//15)\n",
    "cfg[\"data\"][\"shots_per_val\"] = max(1024, cfg[\"data\"].get(\"shots_per_val\", 16384)//16)\n",
    "cfg[\"training\"][\"opt\"] = cfg[\"training\"].get(\"opt\", \"greedy\")  # harmless default\n",
    "\n",
    "print(\"Running with downsized config (edit above for full run)\")\n",
    "metrics = run_from_config(cfg)\n",
    "\n",
    "run_dir = Path(cfg.get(\"output\",{}).get(\"dir\",\".\"))\n",
    "print(\"Artifacts in:\", run_dir.resolve())\n",
    "for p in sorted(run_dir.glob(\"ptnt_*.*\")):\n",
    "    print(\" -\", p.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- FULL RUN (uncomment to enable) ---\n",
    "# cfg = default_config_for_experiment(\"figure3\")\n",
    "# cfg[\"training\"][\"opt\"] = cfg[\"training\"].get(\"opt\", \"auto-hq\")  # if hyper-optimizers installed\n",
    "# metrics = run_from_config(cfg)\n",
    "# print(\"Full run complete. See runs/… for metrics and plots.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c39ba0",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Interpreting outputs\n",
    "You’ll find at least these files in the run directory:\n",
    "- `ptnt_metrics.json` — raw metrics dict.\n",
    "- `ptnt_losses.png` — training vs. validation cross‑entropy curves with **data‑entropy baselines**.\n",
    "- `ptnt_fidelities.png` — (optional) Hellinger fidelities per validation circuit.\n",
    "- `ptnt_fidelities_U.png` — (optional) Full‑U fidelity plot.\n",
    "\n",
    "**Reading the loss plot**\n",
    "- The horizontal dashed lines are the empirical **data entropies** (train/val). They are the **shot‑noise floors**:\n",
    "  the best cross‑entropy one can achieve on the finite‑shot dataset.\n",
    "- A good fit has the **validation** curve trending down and stabilizing **close to** (but not necessarily below) the\n",
    "  val‑data entropy line.\n",
    "- A widening train/val gap → increase data/regularization or reduce model capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "run_dir = Path(cfg.get(\"output\",{}).get(\"dir\",\".\"))\n",
    "mp = run_dir / \"ptnt_metrics.json\"\n",
    "if mp.exists():\n",
    "    metrics = json.loads(mp.read_text())\n",
    "    print(\"Title:\", metrics.get(\"title\"))\n",
    "    print(\"Data entropy:\", metrics.get(\"data_entropy\"), \"Val entropy:\", metrics.get(\"v_data_entropy\"))\n",
    "\n",
    "for name in [\"ptnt_losses.png\", \"ptnt_fidelities.png\", \"ptnt_fidelities_U.png\"]:\n",
    "    p = run_dir / name\n",
    "    if p.exists():\n",
    "        display(Image.open(p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44495c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Practical knobs & troubleshooting\n",
    "- **Contraction optimizer**: `training.opt`\n",
    "  - `greedy` → no dependencies, fast start, decent paths.\n",
    "  - `auto-hq` → best quality when `optuna/nevergrad/cmaes` (and optionally `kahypar`) are installed.\n",
    "  - `hyper-kahypar` → explicitly use kahypar hypergraph partitioner if available.\n",
    "- **Causality** (`training.kappa`): start small (e.g., `1e-3` to `1e-2`) and increase if you see acausal artefacts.\n",
    "- **Ansatz capacity**: vertical/temporal/Kraus bonds in `pt` can be increased as you scale the dataset.\n",
    "- **Backend warning** (“Providing basis_gates with backend is not recommended”): set `device.basis_gates: null`\n",
    "  to let the backend choose its calibrated basis.\n",
    "- **GPU/CPU**: if you want force CPU/GPU, set `JAX_PLATFORMS=cpu|cuda` **before** Python starts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}